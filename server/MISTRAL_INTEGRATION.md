# ğŸ¤– IntegraciÃ³n con Mistral AI - Rubi Assistant

## ğŸ“¦ InstalaciÃ³n

Primero, instala el SDK de Mistral AI:

```bash
npm install @mistralai/mistralai
```

O si usas yarn:

```bash
yarn add @mistralai/mistralai
```

## ğŸ”‘ Obtener tu API Key de Mistral

1. Ve a [https://console.mistral.ai](https://console.mistral.ai)
2. Crea una cuenta o inicia sesiÃ³n
3. Ve a **API Keys** en el menÃº lateral
4. Haz clic en **Create new key**
5. Copia tu API key (empieza con algo como `xxx...`)

## âš™ï¸ ConfiguraciÃ³n en Render

Ve a tu servicio en Render â†’ **Environment** y agrega:

```bash
MISTRAL_API_KEY=tu_api_key_de_mistral_aqui
MISTRAL_MODEL=mistral-large-latest
```

## ğŸ¯ Modelos Disponibles

Mistral ofrece varios modelos segÃºn tus necesidades:

| Modelo | DescripciÃ³n | Uso recomendado | Costo |
|--------|-------------|-----------------|-------|
| `mistral-large-latest` | El mÃ¡s potente | Tareas complejas, razonamiento | $$$ |
| `mistral-medium-latest` | Equilibrado | Uso general | $$ |
| `mistral-small-latest` | RÃ¡pido y econÃ³mico | Tareas simples | $ |
| `mistral-tiny` | Ultra rÃ¡pido | Respuestas cortas | Â¢ |
| `codestral-latest` | Especializado en cÃ³digo | GeneraciÃ³n de cÃ³digo | $$ |

## ğŸ’» Uso BÃ¡sico

### Ejemplo 1: Chat Simple

```typescript
import { mistralChatService } from './mistralChatService';

const response = await mistralChatService.generateChatCompletion([
  {
    role: 'system',
    content: 'Eres Rubi, una asistente personal amigable y servicial.'
  },
  {
    role: 'user',
    content: 'Â¿QuÃ© tareas tengo pendientes hoy?'
  }
]);

console.log(response.content);
```

### Ejemplo 2: Streaming (Respuesta en tiempo real)

```typescript
import { mistralChatService } from './mistralChatService';

const stream = mistralChatService.streamChatCompletion([
  {
    role: 'system',
    content: 'Eres Rubi, una asistente personal.'
  },
  {
    role: 'user',
    content: 'Escribe un email profesional para mi jefe.'
  }
]);

for await (const chunk of stream) {
  process.stdout.write(chunk); // Imprime en tiempo real
}
```

### Ejemplo 3: Con Opciones Personalizadas

```typescript
const response = await mistralChatService.generateChatCompletion(
  [
    { role: 'system', content: 'Eres un asistente tÃ©cnico experto.' },
    { role: 'user', content: 'Explica quÃ© es TypeScript' }
  ],
  {
    temperature: 0.3,  // MÃ¡s determinÃ­stico (0-1)
    maxTokens: 500,    // LÃ­mite de tokens
    model: 'mistral-large-latest'
  }
);
```

## ğŸ”§ IntegraciÃ³n en tus Rutas

### En `chatRoutes.ts`:

```typescript
import { mistralChatService } from './mistralChatService';
import { storage } from './storage';

app.post("/api/chat", isAuthenticated, async (req: any, res) => {
  try {
    const userId = req.user.claims.sub;
    const { message, conversationId } = req.body;

    // Obtener preferencias del usuario
    const prefs = await storage.getUserPreferences(userId);
    
    // Personalizar sistema segÃºn preferencias
    const systemPrompt = `Eres Rubi, una asistente personal ${prefs?.communicationStyle || 'amigable'}. 
    Modo de respuesta: ${prefs?.responseMode || 'balanced'}.
    ${prefs?.userContext ? `Contexto del usuario: ${prefs.userContext}` : ''}`;

    // Obtener historial de conversaciÃ³n
    const history = await storage.getConversationHistory(conversationId);
    
    const messages = [
      { role: 'system', content: systemPrompt },
      ...history,
      { role: 'user', content: message }
    ];

    // Generar respuesta
    const response = await mistralChatService.generateChatCompletion(
      messages,
      {
        temperature: 0.7,
        maxTokens: 1000,
        model: prefs?.isPremium ? 'mistral-large-latest' : 'mistral-small-latest'
      }
    );

    // Guardar en base de datos
    await storage.saveMessage(conversationId, 'user', message);
    await storage.saveMessage(conversationId, 'assistant', response.content);

    res.json({ 
      message: response.content,
      usage: response.usage
    });

  } catch (error) {
    console.error('Chat error:', error);
    res.status(500).json({ error: 'Failed to process chat' });
  }
});
```

## ğŸ¨ PersonalizaciÃ³n segÃºn Usuario

```typescript
// Adaptar la personalidad de Rubi segÃºn preferencias premium
const getSystemPrompt = (prefs: UserPreferences) => {
  const baseName = prefs.customRubiName || 'Rubi';
  const personality = prefs.customRubiPersonality || 
    'Soy una asistente personal inteligente y empÃ¡tica';
  const tone = prefs.customRubiTone || 'friendly';
  
  const toneInstructions = {
    friendly: 'Usa un tono cÃ¡lido y cercano',
    professional: 'MantÃ©n un tono profesional y formal',
    playful: 'Usa humor y emoticones ocasionalmente',
    motivational: 'SÃ© inspiradora y motivadora',
    sarcastic: 'Usa sarcasmo ligero cuando sea apropiado',
    serious: 'SÃ© directa y concisa'
  };

  return `Eres ${baseName}. ${personality}. ${toneInstructions[tone]}.`;
};
```

## ğŸ“Š Costos Estimados

Precios aproximados (verificar en [mistral.ai/pricing](https://mistral.ai/pricing)):

- **mistral-large**: ~$8 / 1M tokens
- **mistral-medium**: ~$2.5 / 1M tokens
- **mistral-small**: ~$1 / 1M tokens
- **mistral-tiny**: ~$0.25 / 1M tokens

Ejemplo: Una conversaciÃ³n tÃ­pica (100 tokens prompt + 200 tokens respuesta):
- Con `mistral-large`: ~$0.0024
- Con `mistral-small`: ~$0.0003

## ğŸ”’ LÃ­mites y Rate Limiting

Para evitar abusos, implementa rate limiting:

```typescript
import rateLimit from 'express-rate-limit';

const chatLimiter = rateLimit({
  windowMs: 15 * 60 * 1000, // 15 minutos
  max: 50, // 50 requests por ventana (usuarios gratis)
  message: 'Demasiadas peticiones, intenta de nuevo mÃ¡s tarde'
});

// Usuarios premium: lÃ­mite mÃ¡s alto
const premiumChatLimiter = rateLimit({
  windowMs: 15 * 60 * 1000,
  max: 200, // 200 requests para premium
  message: 'LÃ­mite alcanzado, intenta de nuevo mÃ¡s tarde'
});

app.post("/api/chat", isAuthenticated, async (req: any, res) => {
  const prefs = await storage.getUserPreferences(req.user.claims.sub);
  const limiter = prefs?.isPremium ? premiumChatLimiter : chatLimiter;
  
  limiter(req, res, async () => {
    // Tu lÃ³gica de chat aquÃ­
  });
});
```

## ğŸ› Manejo de Errores

```typescript
try {
  const response = await mistralChatService.generateChatCompletion(messages);
  return response;
} catch (error: any) {
  if (error.message?.includes('rate limit')) {
    throw new Error('Has alcanzado el lÃ­mite de peticiones. Intenta mÃ¡s tarde.');
  } else if (error.message?.includes('API key')) {
    console.error('Invalid Mistral API key');
    throw new Error('Error de configuraciÃ³n del servidor');
  } else if (error.message?.includes('content policy')) {
    throw new Error('Tu mensaje viola las polÃ­ticas de contenido');
  } else {
    console.error('Mistral error:', error);
    throw new Error('Error al procesar tu mensaje');
  }
}
```

## ğŸ“ Mejores PrÃ¡cticas

1. **Cache**: Guarda respuestas comunes para reducir costos
2. **Streaming**: Usa streaming para mejor UX en respuestas largas
3. **Fallback**: Ten un mensaje por defecto si Mistral falla
4. **Logs**: Registra el uso de tokens para monitorear costos
5. **Context**: Limita el historial de conversaciÃ³n a Ãºltimos 10 mensajes

## ğŸš€ Despliegue

AsegÃºrate de tener en tu `package.json`:

```json
{
  "dependencies": {
    "@mistralai/mistralai": "^1.0.0"
  }
}
```

Y en tus variables de entorno en Render:

```bash
MISTRAL_API_KEY=tu_clave_aqui
MISTRAL_MODEL=mistral-large-latest
```

Â¡Listo! Ahora Rubi usarÃ¡ Mistral AI para generar respuestas inteligentes. ğŸ‰
